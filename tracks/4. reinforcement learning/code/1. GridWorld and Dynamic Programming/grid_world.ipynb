{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "grid_world.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNq0_uEOB7AW",
        "colab_type": "text"
      },
      "source": [
        "# Grid World Example\n",
        "> A gridworld is a small game where the player moves in a 2D grid and tries to reach a certain goal cell at the end in order to win.\n",
        "\n",
        "> This example concerns itself with dynamic programming (dp) methods in reinforcement learning.\n",
        "\n",
        "> In this code only policy evaluation and iteration are implemented. You are expected to implement value iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3C_6ja3CA7s",
        "colab_type": "text"
      },
      "source": [
        "## Imports\n",
        "- numpy to handle arrays and other mathematical operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H7ncIET24kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pupDVLllCEg6",
        "colab_type": "text"
      },
      "source": [
        "## Global Variables\n",
        "- Actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR55L_ck3WJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MOVE_UP    = 0\n",
        "MOVE_DOWN  = 1\n",
        "MOVE_LEFT  = 2\n",
        "MOVE_RIGHT = 3\n",
        "ACTIONS = (\n",
        "    MOVE_UP,\n",
        "    MOVE_DOWN,\n",
        "    MOVE_LEFT,\n",
        "    MOVE_RIGHT\n",
        ")\n",
        "ACTION_NAME = {\n",
        "    MOVE_UP: \"UP\",\n",
        "    MOVE_DOWN: \"DOWN\",\n",
        "    MOVE_LEFT: \"LEFT\",\n",
        "    MOVE_RIGHT: \"RIGHT\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzwRVYf_E0Tb",
        "colab_type": "text"
      },
      "source": [
        "## Environment Class (GridWorld)\n",
        "A class containing the MDP representing the gridworld problem.\n",
        "\n",
        "|Parmeter|Description|\n",
        "|-------------|-------------------------------------------------------------------------------------------------------|\n",
        "|shape|Tuple containing the shape of the grid (row, column)|\n",
        "|final_idx|Tuple containing index of final row and final column|\n",
        "|num_states|Number of states in the MDP|\n",
        "|num_actions|Number of actions in the MDP|\n",
        "|rewards|A grid containing the rewards for each state.<br/> By default the first and last cells of the grid are +1.|\n",
        "|start_state|The index of the state where the agent starts.<br/>By default the first cell of the grid.|\n",
        "|discount|The discount factor for the MDP.<br/>By default 0.9.|\n",
        "|terminals|List of all the terminal states. <br/>By default it contains the first cell of the grid.|\n",
        "|\\_v|The current known value function of the MDP. Initially all 0s.|\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3zY9sGAE5gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GridWorld:\n",
        "    def __init__(self, shape=(4, 4), rewards_grid=None, start=(0, 0), discount_factor=0.9, terminals=[(0, 0)]):\n",
        "        \"\"\"Initialize a GridWorld environment\n",
        "    \n",
        "        Parameters\n",
        "        ----------\n",
        "        shape: tuple\n",
        "          shape of grid\n",
        "        rewards_grid: numpy.ndarray\n",
        "          reward function\n",
        "        start: tuple\n",
        "          start state\n",
        "        discount_factor: float\n",
        "          value between 0 and 1 to dicount future rewards\n",
        "        terminals: list\n",
        "          array containing all terminal states\n",
        "        \"\"\"\n",
        "        # Error handling\n",
        "        if not isinstance(shape, tuple) or not len(shape) == 2\\\n",
        "        or not isinstance(start, tuple) or not len(start) == 2:\n",
        "            raise ValueError('shape and start arguments must be a tuple of length 2')\n",
        "        if discount_factor >= 1 or discount_factor <= 0:\n",
        "            raise ValueError('discount_factor must be a number between 0 and 1')\n",
        "        if rewards_grid is not None and rewards_grid.shape != shape:\n",
        "            raise ValueError('rewards grid shape is not compatible with grid shape')\n",
        "\n",
        "        # Metadata parameters\n",
        "        self.shape  = shape\n",
        "        self.final_idx = (shape[0] - 1, shape[1] - 1)\n",
        "\n",
        "        # MDP variables (assume environment with no randomness)\n",
        "        self.num_states  = np.prod(shape)\n",
        "        self.num_actions = len(ACTIONS)\n",
        "        self.rewards     = rewards_grid\n",
        "        self.start_state = start\n",
        "        self.discount    = discount_factor\n",
        "        self.terminals   = terminals\n",
        "\n",
        "        # Set rewards in case of none.\n",
        "        if rewards_grid is None:\n",
        "            self.rewards = np.zeros(self.shape)\n",
        "            self.rewards[self.start_state] = 1\n",
        "            self.rewards[self.final_idx]   = 1\n",
        "\n",
        "        # Learning parameters\n",
        "        self._v = np.zeros(self.shape)\n",
        "    def reset_v(self):\n",
        "        \"\"\"Reset the currently known value function.\n",
        "        \"\"\"\n",
        "        self._v = np.zeros(self.shape)\n",
        "  \n",
        "    def step(self, action, state):\n",
        "        \"\"\"Take an action in a state and return the result\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        action: int\n",
        "            The action to be taken\n",
        "        state: tuple\n",
        "            The state to take the action in\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            The state the agent ends up in when taking the action\n",
        "        \"\"\"\n",
        "        curr_x = state[1]\n",
        "        curr_y = state[0]\n",
        "        if action == MOVE_UP:\n",
        "            curr_y -= 1\n",
        "        elif action == MOVE_DOWN:\n",
        "            curr_y += 1\n",
        "        elif action == MOVE_LEFT:\n",
        "            curr_x -= 1\n",
        "        elif action == MOVE_RIGHT:\n",
        "            curr_x += 1\n",
        "        else:\n",
        "            raise ValueError('Action not available')\n",
        "    \n",
        "        # Make sure the resulting state is within bounds\n",
        "        curr_y = min(self.final_idx[0], max(0, curr_y))\n",
        "        curr_x = min(self.final_idx[1], max(0, curr_x))\n",
        "        return (curr_y, curr_x)\n",
        "  \n",
        "    def evaluate_policy(self, policy, theta=0.01):\n",
        "        \"\"\"An implementation of the policy evaluation algorithm\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        policy: numpy.ndarray\n",
        "            policy to be evaluated\n",
        "        theta: float\n",
        "            Threshold for stopping the iteration\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            value function based on current policy\n",
        "        \"\"\"\n",
        "        v = np.zeros(self.shape)\n",
        "        eps = theta + 1\n",
        "        while eps > theta:\n",
        "            eps = 0\n",
        "            it = np.nditer(self.rewards, flags=['multi_index'])\n",
        "            while not it.finished:\n",
        "                s = it.multi_index\n",
        "                if s in self.terminals:\n",
        "                    it.iternext()\n",
        "                    continue\n",
        "                val = v[s]\n",
        "                res = 0\n",
        "                for i in range(self.num_actions):\n",
        "                    s_ = self.step(i, s)\n",
        "                    idx = (s[0], s[1], i)\n",
        "                    res += policy[idx] * (self.rewards[s_] + self.discount * self._v[s_])\n",
        "                v[s] = res\n",
        "                eps = max(eps, abs(val - v[s]))\n",
        "                it.iternext()\n",
        "            self._v = v\n",
        "        return v\n",
        "  \n",
        "    def policy_iteration(self):\n",
        "        \"\"\"An implementation of the policy iteration algorithm\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray\n",
        "            learned policy for the agent\n",
        "        \"\"\"\n",
        "        pi = np.full((5, 5, 4), 0.25)\n",
        "        stable = False\n",
        "        while not stable:\n",
        "            stable = True\n",
        "            v = self.evaluate_policy(pi)\n",
        "            it = np.nditer(self.rewards, flags=['multi_index'])\n",
        "            while not it.finished:\n",
        "                s = it.multi_index\n",
        "                if s in self.terminals:\n",
        "                    it.iternext()\n",
        "                    continue\n",
        "                acts = pi[s]\n",
        "                new_vals = []\n",
        "                for i in range(self.num_actions):\n",
        "                    s_ = self.step(i, s)\n",
        "                    new_vals.append(self.rewards[s_] + self.discount * self._v[s_])\n",
        "                best = np.argmax(new_vals)\n",
        "                new_acts = [1 if i == best else 0 for i in range(self.num_actions)]\n",
        "                idx = (s[0], s[1], best)\n",
        "                if pi[idx] != new_acts[best]:\n",
        "                    stable = False\n",
        "                pi[s] = new_acts\n",
        "                it.iternext()\n",
        "        return pi\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZLNTydP31sn",
        "colab_type": "text"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq-AcnlTpCe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rewards = np.array([\n",
        "    [1, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, -1]])\n",
        "terminals = [(0, 0), (4, 4)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEHCQrIIFyMK",
        "colab_type": "code",
        "outputId": "27a9e02b-bef5-410a-d58b-154f70209e2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "env = GridWorld((5, 5), rewards_grid=rewards, terminals=terminals)\n",
        "env.reset_v()\n",
        "pi = env.policy_iteration()\n",
        "print(env._v)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.       1.       0.9      0.81     0.729   ]\n",
            " [1.       0.9      0.81     0.729    0.6561  ]\n",
            " [0.9      0.81     0.729    0.6561   0.59049 ]\n",
            " [0.81     0.729    0.6561   0.59049  0.531441]\n",
            " [0.729    0.6561   0.59049  0.531441 0.      ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}